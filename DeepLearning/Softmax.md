# 多分类与Softmax

## 动机

当我们面对多分类问题时，我们通常需要获得样本对于多个标签的概率，即 $P(\text{label} \mid \mathbf{x})$ 并通过比对不同标签的概率获得最终标签。

对于二分类问题是比较容易解决的，我们可以让神经网络输出一个标量，其取值为正标签的概率，以此我们来判断标签类别。

而对于多分类上述技巧就显得黔驴技穷了。

## 解决方案

这时候我们就可以让神经网络对于每个标签各输出一个值并比对这些值来获得最可能的分类，即

$$
\hat{y}=\argmax_{o_l} o_L
$$

如果我们想通过这几个值获取其概率，聪明的你一定想到可以将其总和加起来并计算对应权重，即：

另对于标签 $l' \in L$ 的输出值为 $o_{l'}$，则：

$$
P(l' \mid x) = \frac{o_{l'}}{\sum_{l \in L} o_l}
$$

分母是所有标签的输出值总和，而分子则是标签的输出值。

这个方法很棒，但是在实践中却存在问题：缺失梯度。即我们无法通过这个方法来训练神经网络。

## Softmax

为了解决上述问题，我们引入了Softmax函数，其定义如下：

$$
P(l' \mid x) = \frac{\exp(o_{l'})}{\sum_{l \in L} \exp(o_l)}
$$

其中 $\exp$ 是指数函数，即 $\exp(x) = e^x$。
