# 2.1 线性回归

我们定义一个线性方程
$$
f(\mathbf{x})=\mathbf{w}^T\mathbf{x}
$$

$$
\mathcal{D}= \{(\mathbf{x}_1,y_1),\ldots,(\mathbf{x}_n,y_n)\}
$$

$$
\mathcal{X} : \mathbb{R}^d\\
\mathcal{Y} : \mathbb{R}\\
$$

$$
f(\mathbf{x}) : \mathcal{X} : \mathbb{R}^d \rightarrow
                \mathcal{Y} : \mathbb{R}\\
\mathbf{x} \in \mathbb{R}^d, \mathbf{w} \in \mathbb{R}^{d+1}, y \in \mathbb{R}
$$

## 成本函数

那我们应该如何去寻找到最终最佳的$\mathbf{w}$呢？我们需要定义一个函数，叫损失函数。其用于衡量我们的预测值与真实值之间的差距。如果我们能通过某些手段最小化这个损失函数，我们也就相当于让模型可以学习到最佳的$\mathbf{w}$。这个过程就是我们所说的训练过程。

而线性回归模型的损失应该怎么定义呢？相信聪明的朋友已经知道了！我们可以通过计算预测值与真实值之间的差距来定义损失函数。我们可以考虑 $y_i$ 与模型预测的 $\hat{y}_i$ 的距离，即 $dist(y, \hat{y})$。

如果我们的模型预测的值与真实值之间的距离越小，那么我们的模型就越好。因此我们可以定义距离函数为这两个点的几何距离，即欧式距离：

$$
\begin{align}
dist(y, \hat{y}) &= |y - \hat{y}|
\\
&=\sqrt{(y - \hat{y})^2}
\end{align}
$$

其中我们将其转化为平方的形式，是因为平方相比于绝对值更容易求导，而且在求导的时候也不会受到绝对值的影响。

而上述式在最后进行平方根运算，而我们在求导的时候会遇到开方的问题，这会使得我们的求导变得复杂。因此我们可以将其转化为平方的形式，是因为平方相比于开方更容易求导，而且在求导的时候也不会受到开方的影响。而其与几何距离函数有着相同的单调性，这也意味着我们可以通过最小化平方距离 $dist'$ 来最小化几何距离。

因此我们可以定义距离函数为：

$$
dist'(y, \hat{y}) = (y - \hat{y})^2
$$

而考虑上式只考虑了一个样本，而我们的数据集中有多个样本，因此我们需要对所有的样本求平均值以得到平均损失。因此我们可以定义损失函数 $L$ 为：

$$
\begin{align}
L(y, \hat{y}) &= \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2\\
 &= \frac{1}{n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x})^2\\
\end{align}
$$

因此目前我们尝试寻找一个权重 $\mathbf{w}$ ，使其最小化成本函数
$$
\min_\mathbf{w} \frac{1}{n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x})^2\\
$$
为了方便后面的数学操作，我们可以将整个函数除以2，这样在后续求导的时候可以消去2，方便计算。而考虑乘上是一个正的常数，不会影响其的单调性与求导，也不会影响最小化这个式子的权重，因此我们可以将目标函数改写为
$$
\min_\mathbf{w} \frac{1}{n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x})^2\\
\downarrow\\
\min_\mathbf{w} \frac{1}{2n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x})^2\\
J=\frac{1}{2n}\sum_{i=1}^{n}(y_i - \mathbf{w}^T\mathbf{x})^2
$$

## 优化目标函数

在完成问题的定义后，我们的目标转化成了最小化成本函数。在高中数学中，我们可以知道：一个函数的极值点只存在于导数为 0 的点，因此我们可以对成本函数求其对导数（更严谨来说，是对于成本函数的参数$\mathbf{w}$求偏导）。

$$
\begin{align}

\frac{\partial L}{\partial \mathbf{w}} &=
\frac{\partial}{\partial \mathbf{w}}    \frac{1}{2n}\sum_{i=1}^{n}(\mathbf{w}^T\mathbf{x}_i - y_i)
\\
&= \frac{\partial}{\partial \mathbf{w}}    \frac{1}{2n}\sum_{i=1}^{n}(\mathbf{w}^T\mathbf{x}_i\mathbf{w}^T\mathbf{x}_i + y_i^2-2\mathbf{w}^T\mathbf{x}_iy_i)
\\
&=\frac{1}{2n}\sum_{i=1}^{n}(2\mathbf{x}_i^2\mathbf{w}-2\mathbf{x}_iy_i)
\\
&=\frac{1}{n}\sum_{i=1}^{n}(\mathbf{x}_i^2\mathbf{w}-\mathbf{x}_iy_i)

\end{align}
$$

令其偏导值为 0:

$$
\begin{align}

\frac{\partial L}{\partial \mathbf{w}}&=0\\
\frac{1}{n}\sum_{i=1}^{n}(\mathbf{x}_i^2\mathbf{w}-\mathbf{x}_iy_i)&=0\\
\sum_{i=1}^{n}(\mathbf{x}_i^2\mathbf{w}-\mathbf{x}_iy_i)&=0\\
\sum_{i=1}^{n}(\mathbf{x}_i^2\mathbf{w})-\sum_{i=1}^{n}(\mathbf{x}_iy_i)&=0\\
\mathbf{w}\sum_{i=1}^{n}(\mathbf{x}_i^2)-\sum_{i=1}^{n}(\mathbf{x}_iy_i)&=0\\
\mathbf{w}&=\frac{\sum_{i=1}^{n}(\mathbf{x}_iy_i)}{\sum_{i=1}^{n}(\mathbf{x}_i^2)}

\end{align}
$$

通过上述优化，我们可以获得最佳的$\mathbf{w}$，这个过程就是我们所说的训练过程。我们可以通过这个过程来训练我们的模型，使得模型可以学习到最佳的$\mathbf{w}$，从而可以对未知的数据进行预测。