# 2.6 支持向量机【待完成】

支持向量机（Support Vector Machine, SVM）是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化。SVM 通过间隔最大化，可以使得模型对噪声更加鲁棒，从而提高模型的泛化能力。

## 线性可分支持向量机

和逻辑回归一样，支持向量机是一个二分类模型，且其也是个线性模型，即其分割超平面为 $\mathbf{w}^T\mathbf{x}+b=0$。我们可以将数据集分为两类，一类为正类，一类为负类。

**线性可分**是指存在一个超平面可以将数据集完全分割开，即正类在超平面的一侧，负类在超平面的另一侧。

在下面我们将首先讨论线性可分情况下的支持向量机。再介绍线性不可分的情况。

## 点到平面的距离

在一切开始前，我需要先介绍一下点到（超）平面（后简称平面）的距离。

我们定义平面 $H$ 为 $\mathbf{w}^T\mathbf{x}+b=0$。
并定义点 $Q$ $(q_1, q_2, q_3,..., q_N)$ 到平面 $H$ 的距禽为 $\text{dist}(Q, H)$。

我们想要知道怎么计算点 $Q$ 到平面 $H$ 的距离。

取平面任意一点 $P$ $(p_1, p_2, p_3,..., p_N)$，且因为平面上的点满足 $\mathbf{w}^T\mathbf{x}+b=0$，因此有 $\mathbf{w}^T\mathbf{p}+b=0$。

向量 $\overrightarrow{PQ} = \mathbf{q} - \mathbf{p}$，其相对于平面法向量的投影为 $| \overrightarrow{PQ}|\cos \theta$，其中 $\theta$ 为 $\overrightarrow{PQ}$ 和 $\mathbf{w}$ 的夹角。考虑其投影的几何性质，我们可以得到 $\text{dist}(Q, H) = | \overrightarrow{PQ}|\cos \theta$。

令法向量 $\mathbf{n}\perp H: (w_1, w_2, ..., w_n)$，有



$$
\begin{align}
\overrightarrow{PQ}\cdot \mathbf{n} &= |\overrightarrow{PQ}||\mathbf{n}|\cos \theta\\

|\overrightarrow{PQ}|\cos \theta &= \frac{\mathbf{n}}{|\mathbf{n}|} \cdot \overrightarrow{PQ} \\
\end{align}
$$

因此则有：

$$
\begin{align}
\text{dist}(Q, H) &= \frac{\mathbf{n}}{|\mathbf{n}|} \cdot \overrightarrow{PQ} \\
&= \frac{\mathbf{w}}{||\mathbf{w}||} \cdot (\mathbf{q} - \mathbf{p})\\
&= \frac{1}{||\mathbf{w}||} \cdot (\mathbf{w}^T\mathbf{q} - \mathbf{w}^T\mathbf{p})\\
&= \frac{1}{||\mathbf{w}||} \cdot (\mathbf{w}^T\mathbf{q} + b - (\mathbf{w}^T\mathbf{p}+b))\\
\because \quad& \mathbf{w}^T\mathbf{p}+b=0\\
\therefore \quad \text{dist}(Q, H) &= \frac{1}{||\mathbf{w}||} \cdot (\mathbf{w}^T\mathbf{q} + b)\\
&= \frac{\mathbf{w}^T\mathbf{q} + b}{||\mathbf{w}||}

\end{align}

$$


## 定义间隔（Margin）

$$
\begin{equation}
h(\mathbf{x})=\left\{
\begin{aligned}
+1 \quad&\text{ if }\mathbf{w}^T\mathbf{x}+b>0 \\
-1 \quad&\text{ if }\mathbf{w}^T\mathbf{x}+b<0
\end{aligned}
\right.
\end{equation}
$$

定义距离公式

$$
\begin{align}
&\text{dist}(h, \mathbf{x}_i) = \frac{\mid h(\mathbf{x}_i) \mid}{||\mathbf{w}||}\\
s.t. &\quad |\mathbf{w}^T\mathbf{x}_i + b| \geq 0
\end{align}
$$

我们定义间隔

$$
\begin{align}
\gamma &= \text{dist}(h, \mathbf{x}_i) \\
&=\frac{\mid h(\mathbf{x}_i) \mid}{||\mathbf{w}||}
\end{align}
$$

我们定义所有点都被正确分类，即

$$
\begin{equation}
y_i=\left\{
\begin{aligned}
+1 \quad&\text{ if }\mathbf{w}^T\mathbf{x}_i + b>0 \quad \text{分类正确}\\
-1 \quad&\text{ if }\mathbf{w}^T\mathbf{x}_i + b<0 \quad \text{分类正确}\\
+1 \quad&\text{ if }\mathbf{w}^T\mathbf{x}_i + b\leq0 \quad \text{分类错误} \\
-1 \quad&\text{ if }\mathbf{w}^T\mathbf{x}_i + b\geq0 \quad \text{分类错误}
\end{aligned}
\right.
\end{equation}
$$

即如果分类正确，$y_i(\mathbf{w}^T\mathbf{x}_i+b)>0$，如果分类错误，$y_i(\mathbf{w}^T\mathbf{x}_i +b )\leq0$。考虑 $y_i \in \{ +1, -1\}$，即乘上它只会改变 $\mathbf{w}^T\mathbf{x}_i + b$ 的正负号而不会改变其绝对值，因此如果所有点都被正确分类，我们可以把边距改写为

$$
\begin{align}
{\mid \mathbf{w}^T\mathbf{x}_i + b\mid}
&\rightarrow
y_i(\mathbf{w}^T\mathbf{x}_i+b) > 0\\

&\Downarrow\\

\gamma_i
&=\frac{\mid h(\mathbf{x}_i) \mid}{||\mathbf{w}||} \\
&=\frac{y_i(\mathbf{w}^T\mathbf{x}_i+b)}{||\mathbf{w}||}
\end{align}
$$

$$
\begin{align}
s.t. \quad \forall (\mathbf{x}_i, y_i)\in \mathcal{D}.\quad y_i h(\mathbf{x}_i)>0
\end{align}
$$




## 定义原问题

最小化距离
$$
\begin{align}
& \argmax_{\mathbf{w}, b}\{\min_n \gamma_n\}\\
\text{where}
&\quad \gamma_n = \frac{1}{||\mathbf{w}||}\\
s.t. 
&\quad \forall (\mathbf{x}_i, y_i)\in \mathcal{D}.\quad y_i h(\mathbf{x}_i)>0
\end{align}
$$



如果我们定义间距的物理距离为 $1$，即对于支持向量，我们有：

$$
\begin{align}
\gamma_i &= \frac{| \mathbf{w}^T\mathbf{x}_i |}{\|\mathbf{w}\|} \\
&\downarrow \\
\gamma_i &= \frac{1}{\|\mathbf{w}\|}
\end{align}
$$
因此可以改写问题
$$
\begin{align}
& \argmax_{\mathbf{w}, b}\left\{\min_n \frac{1}{||\mathbf{w}||}\right\}\\
s.t. 
\quad& \forall (\mathbf{x}_i, y_i)\in \mathcal{D}.\quad y_i h(\mathbf{x}_i)>0
\end{align}
$$

我们对 $\mathbf{x}$ 进行非线性变换，即 $\mathbf{x}\rightarrow \phi(\mathbf{x})$，这样我们可以得到

$$
\begin{align}

& \argmin_{\mathbf{w}, b}\left\{\frac{1}{2}|| \mathbf{w} ||^2\right\}\\
s.t. \quad& 
\forall (\mathbf{x}_i, y_i)\in \mathcal{D}.\\
&y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b)\geq1
\end{align}
$$



## 改写为对偶问题

考虑原问题

$$
\begin{align}

& \argmin_{\mathbf{w}, b}\left\{\frac{1}{2}|| \mathbf{w} ||^2\right\}\\
s.t. \quad& 
\forall (\mathbf{x}_i, y_i)\in \mathcal{D}.\\
&y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b)\geq1
\end{align}
$$


我们可以定义一个惩罚函数 $g(\mathbf{w}, b)$，当违反约束时，这个惩罚会很大，而当满足约束时，这个惩罚会很小。因此在优化这个整体问题时，我们也在优化惩罚函数，即尽可能符合约束。即我们可以定义把原问题重新写为：

$$
\begin{align}
& \argmin_{\mathbf{w}, b}\left\{\frac{1}{2}|| \mathbf{w} ||^2 + g(\mathbf{w}, b)\right\}
\end{align}
$$
这样我们就可以把原问题转化为一个无约束问题。

我们首先改写约束条件：

$$
\begin{align}
y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b)&\geq1\\
&\Downarrow\\
y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b)-1&\geq0\\
&\Downarrow\\
1-y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b)&\leq0
\end{align}
$$

这次改写使得约束条件变为了一个不等式，xxxxx


我们可以定义惩罚函数为

$$
\begin{align}
& g(\mathbf{w}, b) = \sum_{i=1}^N \alpha_i(1-y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b))\\
s.t. \quad& \alpha_i \geq 0
\end{align}
$$
我们称 $\alpha_i$ 为拉格朗日乘子。且其：

1. 当 $\mathbf{x}_i$ 不是支持向量，即 $\alpha_i = 0$，这个点对于最终的分类没有影响
2. 当 $\mathbf{x}_i$ 是支持向量，即 $\alpha_i > 0$，这个点对于最终的分类有影响

当 $\alpha_i > 0$ 时，我们称 $\mathbf{x}_i$ 为支持向量。需要注意的是尽管有可能 $\mathbf{x}_i$ 在边界上，但是其 $\alpha_i$ 仍然可能为 $0$。

我们希望使得所有违反约束的点的惩罚尽可能大，即我们希望最大化这些惩罚，因此我们可以改写惩罚函数为：

$$
\begin{align}
& g(\mathbf{w}, b) = \max \sum_{i=1}^N \alpha_i(1-y_i(\mathbf{w}^T\phi(\mathbf{x}_i)+b))\\
s.t. \quad& \alpha_i \geq 0
\end{align}
$$

xxxx
$$
\begin{align}
&\argmin_{\mathbf{w}, b}\left\{\frac{1}{2}|| \mathbf{w} ||^2+
\max_\alpha\sum_{n=1}^N\alpha_n(1-y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b))
\right\}\\
s.t. \quad
&y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b)\geq1\\
&\alpha_n\geq0
\end{align}
$$

$$
\begin{align}
&\min_{\mathbf{w}, b} \max_\alpha\left\{\frac{1}{2}|| \mathbf{w} ||^2+
\sum_{n=1}^N\alpha_n(1-y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b))
\right\}\\
s.t. \quad
&y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b)\geq1\\
&\alpha_n\geq0
\end{align}
$$

$$
\begin{align}
&\max_\alpha\min_{\mathbf{w}, b} \left\{\frac{1}{2}|| \mathbf{w} ||^2+
\sum_{n=1}^N\alpha_n(1-y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b))
\right\}\\
s.t. \quad
&y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b)\geq1\\
&\alpha_n\geq0
\end{align}
$$

$$
\begin{align}
&\max_\alpha\min_{\mathbf{w}, b}
\mathcal{L}(\alpha, \mathbf{w}, b)=
\left\{\frac{1}{2}|| \mathbf{w} ||^2+
\sum_{n=1}^N\alpha_n(1-y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b))
\right\}\\
s.t. \quad
&y_n(\mathbf{w}^T\phi(\mathbf{x}_n)+b)\geq1\\
&\alpha_n\geq0
\end{align}
$$

我们对其进行求导，即

$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} &= \mathbf{w} - \sum_{n=1}^N \alpha_n y_n \phi(\mathbf{x}_n) = 0\\
\mathbf{w} &= \sum_{n=1}^N \alpha_n y_n \phi(\mathbf{x}_n) \\
\frac{\partial \mathcal{L}}{\partial b} &= -\sum_{n=1}^N \alpha_n y_n = 0\\
\end{align}
$$

将 $\mathbf{w}$ 和 $b$ 代入 $\mathcal{L}$，我们可以把优化问题改写为

$$
\begin{align}
&\argmax_\alpha \mathcal{L}(\alpha)=\sum^N_{n=1}\alpha_n-
\frac{1}{2}\sum^N_{n=1}\sum^N_{m=1}
\alpha_n\alpha_my_ny_m\phi(\mathbf{x}_n)^T\phi(\mathbf{x}_m)\\

s.t.\quad
&\alpha_n\geq 0\\
&\sum^N_{n=1}a_ny_n=0
\end{align}
$$

我们定义核函数 $\kappa(\mathbf{x}_n, \mathbf{x}_m)=\phi(\mathbf{x}_n)^T\phi(\mathbf{x}_m)$，我们可以把优化问题改写为

$$
\begin{align}
&\argmax_\alpha \mathcal{L}(\alpha)=\sum^N_{n=1}\alpha_n-
\frac{1}{2}\sum^N_{n=1}\sum^N_{m=1}
\alpha_n\alpha_my_ny_m\kappa(\mathbf{x}_n, \mathbf{x}_m)\\

s.t.\quad
&\alpha_n\geq 0\\
&\sum^N_{n=1}a_ny_n=0\\
\text{where}\quad
&\kappa(\mathbf{x}_n, \mathbf{x}_m)=\phi(\mathbf{x}_n)^T\phi(\mathbf{x}_m)
\end{align}
$$

## 软间隔 Soft Margin：非线性可分情况


## SMO 算法

## 核函数 Kernel Function


