# Gradient Descent

在上一节，我们通过线性回归了解了基础的监督学习的模型。我们定义了模型，成本函数，并尝试优化成本函数。优化成本函数使得我们的模型能够更好地完成任务（拟合数据）。在这一节，我们将介绍一个常用的优化算法：梯度下降（Gradient Descent）。其是一种更通用的方法用于最小化函数。

## 1. 梯度下降

![](img/gd-right.png)

![](img/gd-left.png)


## 原理：梯度下降为什么总是能走向最深的点

梯度下降的本质推导来源于泰勒展开。泰勒展开是一个非常重要的数学工具，它可以将一个函数在某一点附近用一个多项式来近似表示。泰勒展开的公式如下：

$$
\begin{align}
f(x) 
&=\sum_{n=0}^\infty\frac{f^{(n)}(x_0)}{n!}(x-x^0)^n
\\
&= f(x_0) + f'(x_0)(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
\end{align}


$$

TODO: https://en.wikipedia.org/wiki/Taylor_series 图
其将函数在 $x_0$ 处展开，展开的多项式的次数为 $n$。我们希望近似原函数 $f$ 位于 $x$ 时的值。其有两个非常有用的性质：

1. 当我们需要拟合的点 $x$ 离 $x_0$ 越近，泰勒展开后的结果越接近原函数。
2. 当泰勒展开次数越高，拟合的效果越好。

我们假设 $L$ 为损失函数，并对其对于权重 $\mathbf{w}$ 进行一阶泰勒展开，即：

$$
\begin{align}

L(\mathbf{w}) 
&\approx L(\mathbf{w}_0) + L'(\mathbf{w}_0)(\mathbf{w}-\mathbf{w}_0)

\\
&= L(\mathbf{w}_0) + \nabla L(\mathbf{w}_0)(\mathbf{w}-\mathbf{w}_0)
\end{align}
$$

我们对其进行变换
$$
\begin{align}
L(\mathbf{w}) &\approx L(\mathbf{w}_0) + \nabla L(\mathbf{w}_0)(\mathbf{w}-\mathbf{w}_0)\\
L(\mathbf{w}) - L(\mathbf{w}_0) &\approx \nabla L(\mathbf{w}_0)(\mathbf{w}-\mathbf{w}_0)\\
\end{align}
$$

如果我们认为更新后的权重 $\mathbf{w}$ 是 $\mathbf{w}_0$ 增加一个单位向量 $\mathbf{v}$ 伴有学习率 $\eta$，则可以写成 $\mathbf{w} = \mathbf{w}_0 + \eta \mathbf{v}$。我们可以将上式代入到损失函数的泰勒展开中，得到：

$$
\begin{align}
L(\mathbf{w}) - L(\mathbf{w}_0) &\approx \nabla L(\mathbf{w}_0)(\mathbf{w}-\mathbf{w}_0)\\
L(\mathbf{w}) - L(\mathbf{w}_0) &\approx \nabla L(\mathbf{w}_0)(\mathbf{w}_0 + \eta \mathbf{v}-\mathbf{w}_0)\\
L(\mathbf{w}_0 + \eta \mathbf{v}) - L(\mathbf{w}_0) &\approx \eta \nabla L(\mathbf{w}_0)^T\mathbf{v}\\
\end{align}
$$

我们可以认为 $\nabla L(\mathbf{w}_0)^T\mathbf{v}$ 是损失函数在 $\mathbf{w}_0$ 处的梯度与更新的方向 $\mathbf{v}$ 的点积。

$$
\nabla L(\mathbf{w}_0)^T\mathbf{v} = ||\nabla L(\mathbf{w}_0)||\cdot||\mathbf{v}||\cdot\cos(\theta)
$$

其中 $\theta$ 是 $\nabla L(\mathbf{w}_0)$ 与 $\mathbf{v}$ 的夹角。

考虑单位向量 $\mathbf{v}$，其的膜度为1，即 $||\mathbf{v}||=1$。因此我们可以得到：

$$
\nabla L(\mathbf{w}_0)^T\mathbf{v} = ||\nabla L(\mathbf{w}_0)||\cos(\theta)
$$

如果假设 $\nabla L(\mathbf{w}_0)$ 的膜长不变，则其的点积越大，说明 $\theta$ 越小，也就是说我们的更新方向越接近于梯度的方向。

TODO：

## 拓展：IRLS

如果将泰勒展开展开至二阶，则有

$$
\begin{align}
T_n(\mathbf{w})

&=\sum^n_{k=0}\frac{L^{(k)}(\mathbf{w}_0)}{k!}(\mathbf{w}-\mathbf{w}_0)^k\\

&=
\frac{L^{(0)}(\mathbf{w}_0)}{0!}(\mathbf{w}-\mathbf{\mathbf{w}}_0)^0+
\frac{L^{(1)}(\mathbf{w}_0)}{1!}(\mathbf{w}-\mathbf{\mathbf{w}}_0)^1+
\frac{L^{(2)}(\mathbf{w}_0)}{2!}(\mathbf{w}-\mathbf{\mathbf{w}}_0)^2

\\

&=L(\mathbf{w}_0)+(\mathbf{w}-\mathbf{w}_0)L(\mathbf{w}_0)+\frac{(\mathbf{w}-\mathbf{w}_0)^2}{2}L''(\mathbf{w}_0)
\end{align}

$$

令其导数为0，我们可以得到

$$
\begin{align}
\frac{\partial L}{\partial \mathbf{w}} 
\approx \frac{\partial}{\partial w}
L(\mathbf{w}_0)+(\mathbf{w}-\mathbf{w}_0)L(\mathbf{w}_0)+\frac{(\mathbf{w}-\mathbf{w}_0)^2}{2}L''(\mathbf{w}_0) &=0\\

\frac{\partial}{\partial \mathbf{w}}
L(\mathbf{w}_0)+(\mathbf{w}-\mathbf{w}_0)L(\mathbf{w}_0)+\frac{(\mathbf{w}-\mathbf{w}_0)^2}{2}L''(\mathbf{w}_0)
&=0\\

L'(\mathbf{w}_0)+wL''(\mathbf{w}_0)-\mathbf{w}_0L''(\mathbf{w}_0)&=0\\

L'(\mathbf{w}_0)+(\mathbf{w}-\mathbf{w}_0)L''(\mathbf{w}_0)&=0\\
\mathbf{w}-\mathbf{w}_0&=-\frac{L'(\mathbf{w}_0)}{L''(\mathbf{w}_0)}\\
\mathbf{w}&=\mathbf{w}_0- \frac{L'(\mathbf{w}_0)}{L''(\mathbf{w}_0)}\\
\mathbf{w}'&=\mathbf{w}-\frac{L'(\mathbf{w}_0)}{L''(\mathbf{w}_0)}
\end{align}
$$

即在二阶泰勒展开中，我们认为损失函数 $L$ 的最低点可以由 $\mathbf{w}'=\mathbf{w}-\frac{L'(\mathbf{w}_0)}{L''(\mathbf{w}_0)}$ 得到。而 $\frac{L'(\mathbf{w}_0)}{L''(\mathbf{w}_0)}$ 可以看作是 $\mathbf{w}$ 的更新方向。

因此我们可以定义新的梯度下降函数为

$$
\begin{align}
\mathbf{w} &= \mathbf{w} - \eta \frac{L'(\mathbf{w}_0)}{L''(\mathbf{w}_0)}\\
&=\mathbf{w} - \eta\frac{\nabla L(\mathbf{w}_0)}{H_L(\mathbf{w}_0)}\\
&=\mathbf{w} - \eta H_L^1(\mathbf{w}_0)\nabla L(\mathbf{w}_0)
\end{align}
$$

其中 $H$ 为 Hessian 矩阵，$H_L^1$ 为 Hessian 矩阵的逆矩阵。

TODO

我们将新的更新公式命名为 IRLS（Iteratively Reweighted Least Squares）算法。其是一种迭代的最小二乘法，其在每次迭代中都会重新计算权重。

