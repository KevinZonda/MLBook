# 更多的数学：线性代数

你可以暂时跳过这章，这可能涉及一点超出高中的表达，不过我们依然建议你尝试阅读和学习它。

在未来遇到的时候，我们会再次提及这段知识。

## 向量

高中中，可能不少同学已经接触过向量的概念，不过我们还是简单的复习一下。

向量在数学中表示一个有方向和大小的量，通常在坐标系用一个箭头表示，箭头的长度表示向量的大小，箭头的方向表示向量的方向。与向量相对应的是标量，标量是只有大小而没有方向的量。

> 来自作者的建议：  
> 你可以将向量看作是一个有序的数列，这个数列中的每一个元素都是一个标量。  
> 在机器学习中的绝大多数时间，我们将向量看待成一个存储了 $d$ 个数值的列表，而不去担心其具体的几何意义。

对于一个 $d$ 维的向量 $\mathbf{v}$ 其形式为：

$$
\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix}
$$

在机器学习中，我们通常认为向量是一个列向量，即是竖着的向量，而不是横着的。

> **符号标记**  
> 在高中，我们通常使用 $\overrightarrow{v}$ 或者 $\bar{v}$ 来表示向量，而在机器学习中，我们通常使用 $\mathbf{v}$ （粗体小写） 来表示向量。

向量的长度被定义为几何距离，即每一个维度的平方和的平方根：

$$
\left\| \mathbf{v} \right\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_d^2}
$$

### 基础运算

向量的基础运算包括：

- 向量的加法
- 向量的减法
- 向量的乘法

#### 向量的加法

向量的加法是指两个向量的对应元素相加，即：

$$
\mathbf{v} + \mathbf{u} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix} + \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_d \end{bmatrix} = \begin{bmatrix} v_1 + u_1 \\ v_2 + u_2 \\ \vdots \\ v_d + u_d \end{bmatrix}
$$

#### 向量的减法

向量的减法是指两个向量的对应元素相减，即：

$$
\mathbf{v} - \mathbf{u} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix} - \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_d \end{bmatrix} = \begin{bmatrix} v_1 - u_1 \\ v_2 - u_2 \\ \vdots \\ v_d - u_d \end{bmatrix}
$$

#### 向量的乘法

向量的乘法是一个复杂的系统我们可以将其分为很多种情况。

向量的数乘：标量与向量的乘法是将标量与向量的每一个元素相乘，即：

$$
\alpha \mathbf{v} = \alpha \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix} = \begin{bmatrix} \alpha v_1 \\ \alpha v_2 \\ \vdots \\ \alpha v_d \end{bmatrix}
$$

向量的点乘：点乘是指两个向量对应元素相乘再相加，即：

$$
\mathbf{v} \cdot \mathbf{u} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix} \cdot \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_d \end{bmatrix} = v_1u_1 + v_2u_2 + \cdots + v_du_d
$$

向量的点乘有一个重要的性质，即：

$$
\mathbf{v} \cdot \mathbf{u} = \left\| \mathbf{v} \right\| \left\| \mathbf{u} \right\| \cos\theta
$$

其中 $\theta$ 为两个向量之间的夹角。

> **符号标记**  
> 在高中，我们通常使用 $\cdot$ 来表示点乘，而在机器学习中，我们通常使用 $\mathbf{v}^T\mathbf{u}$ 来表示点乘。我们将在后面的章节中详细介绍这个符号的含义。  
> 有些书籍中也使用 $\langle \mathbf{v}, \mathbf{u}\rangle$ 来表示两个向量的点乘。

#### 向量的转置（Transpose）

看到转置这个词，你可能会很困惑，不过不用担心，这个概念很简单。

对于向量的转置，对于向量来说就是将一个行向量转换为列向量，或者将一个列向量转换为行向量。即：

$$
\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix} \Rightarrow \mathbf{v}^T = \begin{bmatrix} v_1 & v_2 & \cdots & v_d \end{bmatrix}
$$

$$
\mathbf{v}^T = \begin{bmatrix} v_1 & v_2 & \cdots & v_d \end{bmatrix} \Rightarrow {\mathbf{v}^T}^T = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_d \end{bmatrix}
$$

因此我们有一个非常良好的性质：

$$
\mathbf{v} = {\mathbf{v}^T}^T
$$

即转置的转置等于原向量。

